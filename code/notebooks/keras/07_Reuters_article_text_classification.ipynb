{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of text articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links\n",
    "* [Reuters newswire dataset](https://keras.io/datasets/)\n",
    "* [Reuters text classification](https://www.bonaccorso.eu/2016/08/02/reuters-21578-text-classification-with-gensim-and-keras/)\n",
    "* [Keras Reuters MLP example](https://github.com/keras-team/keras/blob/master/examples/reuters_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reuters dataset is made up of 11,228 newswires from Reuters, labeled over 46 topics. The word index used to encode the sequences is stored in the reuters_word_index.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30979 words used to encode.\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "print('There are', len(word_index), 'words used to encode.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "batch_size = 32\n",
    "epochs = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Reuters data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "46 classes\n"
     ]
    }
   ],
   "source": [
    "print('Loading Reuters data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(\n",
    "    num_words=max_words, test_split=0.2)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing sequence data...\n",
      "x_train shape: (8982, 1000)\n",
      "x_test shape: (2246, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing sequence data...')\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8982, 46)\n",
      "y_test shape: (2246, 46)\n"
     ]
    }
   ],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                23598     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 536,110\n",
      "Trainable params: 536,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Building model...')\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8083 samples, validate on 899 samples\n",
      "Epoch 1/5\n",
      "8083/8083 [==============================] - 1s 172us/step - loss: 1.4340 - acc: 0.6764 - val_loss: 1.0881 - val_acc: 0.7642\n",
      "Epoch 2/5\n",
      "8083/8083 [==============================] - 1s 137us/step - loss: 0.7898 - acc: 0.8179 - val_loss: 0.9403 - val_acc: 0.7831\n",
      "Epoch 3/5\n",
      "8083/8083 [==============================] - 1s 137us/step - loss: 0.5490 - acc: 0.8660 - val_loss: 0.8915 - val_acc: 0.8020\n",
      "Epoch 4/5\n",
      "8083/8083 [==============================] - 1s 136us/step - loss: 0.4157 - acc: 0.8999 - val_loss: 0.8749 - val_acc: 0.8065\n",
      "Epoch 5/5\n",
      "8083/8083 [==============================] - 1s 136us/step - loss: 0.3237 - acc: 0.9151 - val_loss: 0.9027 - val_acc: 0.8020\n",
      "2246/2246 [==============================] - 0s 47us/step\n",
      "Test loss: 0.89095894248\n",
      "Test accuracy: 0.791629563669\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
